{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make sure both processes get same audio\n",
    "\n",
    "\n",
    "-choose file\n",
    "- process via diarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_and_process_connections_interval_sec: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_147367/4266564517.py:12: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import logging\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from app.database_redis.connection import get_redis_client\n",
    "from app.services.apis.streamqueue_service.client import StreamQueueServiceAPI\n",
    "from app.services.audio.redis import Connection, Diarizer, Meeting, Transcriber\n",
    "from app.settings import settings\n",
    "from app.services.audio.redis import Diarisation,Transcript\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_client = await get_redis_client(settings.redis_host, settings.redis_port, settings.redis_password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "meeting_id = 'xng-juvg-nyv'\n",
    "diarization = Diarisation(meeting_id,redis_client=redis_client)\n",
    "transcript = Transcript(meeting_id,redis_client=redis_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "meeting = Meeting(redis_client,meeting_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_transcripts(transcription):\n",
    "    df = pd.DataFrame(transcription)\n",
    "    df = df[[2,3,4]]\n",
    "    df.columns = ['start','end','speech']\n",
    "    df['start'] = pd.to_timedelta(df['start'],unit='s') + pd.Timestamp(seek)\n",
    "    df['end'] = pd.to_timedelta(df['end'],unit='s') + pd.Timestamp(seek)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_diarization(df):\n",
    "\n",
    "    df['start'] = pd.to_timedelta(df['start'],unit='s')+pd.Timestamp(seek)\n",
    "    df['end']   = pd.to_timedelta(df['end'],unit='s')+pd.Timestamp(seek)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_diarization(trans_df,diar_df):\n",
    "    segments = trans_df.to_dict('records')\n",
    "\n",
    "    for seg in segments:\n",
    "        diar_df['intersection'] = np.minimum(diar_df['end'], seg['end']) - np.maximum(diar_df['start'], seg['start'])\n",
    "        speaker_ = diar_df[(diar_df['intersection'] == diar_df['intersection'].max())&(diar_df['intersection']>pd.Timedelta(0))]['speaker']\n",
    "        if len(speaker_)>0:\n",
    "            seg['speaker'] = speaker_.iloc[0]\n",
    "            \n",
    "    return pd.DataFrame(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_transcripts(new_transcriptions):\n",
    "    dfs = []\n",
    "    n=0\n",
    "    for t,seek,connection_id in new_transcriptions:\n",
    "        df = pd.DataFrame(t)[[2,3,4]]\n",
    "        df.columns = ['start','end','speech']\n",
    "        df['start'] = pd.to_timedelta(df['start'],unit='s') + pd.Timestamp(seek)\n",
    "        df['end'] = pd.to_timedelta(df['end'],unit='s') + pd.Timestamp(seek)\n",
    "        df['chunk']=n\n",
    "        dfs.append(df)\n",
    "        n+=1\n",
    "        \n",
    "    return pd.concat(dfs).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_diarizations(new_diarizations):\n",
    "    dfs = []\n",
    "    for d,seek,connection_id in new_diarizations:\n",
    "        df = pd.DataFrame(d)\n",
    "        df['start'] = pd.to_timedelta(df['start'],unit='s')+pd.Timestamp(seek)\n",
    "        df['end']   = pd.to_timedelta(df['end'],unit='s')+pd.Timestamp(seek)\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_diarization(trans_df,diar_df):\n",
    "    segments = trans_df.to_dict('records')\n",
    "\n",
    "    for seg in segments:\n",
    "        diar_df['intersection'] = np.minimum(diar_df['end'], seg['end']) - np.maximum(diar_df['start'], seg['start'])\n",
    "        speaker_ = diar_df[(diar_df['intersection'] == diar_df['intersection'].max())&(diar_df['intersection']>pd.Timedelta(0))]['speaker']\n",
    "        if len(speaker_)>0:\n",
    "            seg['speaker'] = speaker_.iloc[0]\n",
    "            \n",
    "    return pd.DataFrame(segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "last_mutable = pd.DataFrame(columns = ['start','end','speech','speaker'])\n",
    "immutable = pd.DataFrame(columns = ['start','end'])\n",
    "mutable = pd.DataFrame(columns = ['start','end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_diarizations = []\n",
    "new_transcriptions = [] \n",
    "diar_df = pd.DataFrame(columns = ['start','end','speaker'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_147367/4103374667.py:18: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  trans_df = pd.concat([trans_df,last_mutable[last_mutable['speaker'].isna()]])\n",
      "/tmp/ipykernel_147367/4103374667.py:22: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  mutable = mutable.replace(rename_dict).sort_values('start')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speech</th>\n",
       "      <th>speaker</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>change</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>А что теперь мне представить? Não desloou?</td>\n",
       "      <td>0</td>\n",
       "      <td>2024-05-25 16:49:17.944605+00:00</td>\n",
       "      <td>2024-05-25 16:49:24.142605+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             speech  speaker  \\\n",
       "change                                                         \n",
       "1        А что теперь мне представить? Não desloou?        0   \n",
       "\n",
       "                                  start                              end  \n",
       "change                                                                    \n",
       "1      2024-05-25 16:49:17.944605+00:00 2024-05-25 16:49:24.142605+00:00  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "while True:\n",
    "    d = await diarization.rpop()\n",
    "    if not d:\n",
    "        break\n",
    "    new_diarizations.append(json.loads(d))\n",
    "print(len(new_diarizations))\n",
    "\n",
    "while True:\n",
    "    d = await transcript.rpop()\n",
    "    if not d:\n",
    "        break\n",
    "    new_transcriptions.append(json.loads(d))\n",
    "print(len(new_transcriptions))\n",
    "#TODO: we want to crop low sertainty 1/2 overlap from each side\n",
    "if len(new_transcriptions)>0:\n",
    "    trans_df = prep_transcripts(new_transcriptions) \n",
    "    diar_df = prep_diarizations(new_diarizations)\n",
    "    trans_df = pd.concat([trans_df,last_mutable[last_mutable['speaker'].isna()]])\n",
    "    mutable = apply_diarization(trans_df,diar_df)\n",
    "    if len(mutable)>0:\n",
    "        rename_dict = {s:n for n,s in enumerate(mutable['speaker'].dropna().unique().tolist())}\n",
    "        mutable = mutable.replace(rename_dict).sort_values('start')\n",
    "        immutable = pd.concat([immutable,last_mutable.dropna()]).reset_index(drop = True)\n",
    "        last_mutable= mutable.copy()\n",
    "\n",
    "            \n",
    "    mutable=mutable.drop_duplicates('start')\n",
    "\n",
    "    mutable['change'] = mutable['speaker']!=mutable['speaker'].shift()\n",
    "    mutable['change'] = mutable['change'].cumsum()\n",
    "mutable.groupby('change').agg({'speech':'sum','speaker':\"first\",'start':'first',\"end\":'last'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
