{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import  Distance, VectorParams\n",
    "from redis.asyncio.client import Redis\n",
    "\n",
    "client = QdrantClient(url=\"http://localhost:6333\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchvision is not available - cannot save figures\n"
     ]
    }
   ],
   "source": [
    "from pathlib import  Path\n",
    "import torch\n",
    "from pyannote.audio import Pipeline\n",
    "import io\n",
    "from audio import AudioSlicer\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "Path.ls = lambda self:[item for item in self.iterdir()]\n",
    "\n",
    "def parse_segment(segment):\n",
    "    return segment[0].start, segment[0].end, int(segment[-1].split(\"_\")[1])\n",
    "\n",
    "\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "            \"pyannote/speaker-diarization-3.1\",\n",
    "            use_auth_token=\"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\",\n",
    "        )\n",
    "pipeline.to(torch.device(\"cuda\"))\n",
    "\n",
    "audio_path = Path('/home/dima/ssd/1/audio')\n",
    "audio_path.exists()\n",
    "\n",
    "\n",
    "from qdrant_client.models import PointStruct\n",
    "from uuid import uuid4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.cluster.hierarchy import linkage, fcluster\n",
    "from qdrant_client.models import PointStruct\n",
    "from uuid import uuid4\n",
    "import redis.asyncio as redis\n",
    "\n",
    "def parse_segment(segment):\n",
    "    return segment[0].start, segment[0].end, int(segment[-1].split(\"_\")[1])\n",
    "\n",
    "def centroid_linkage_distance(matrix):\n",
    "    Z = linkage(matrix, method='centroid')\n",
    "    return Z\n",
    "\n",
    "def compute_centroids(embeddings, labels):\n",
    "    unique_labels = np.unique(labels)\n",
    "    centroids = np.zeros((len(unique_labels), embeddings.shape[1]))\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        centroids[i] = embeddings[labels == label].mean(axis=0)\n",
    "    return centroids, unique_labels\n",
    "\n",
    "def map_labels_to_centroid_indices(labels, unique_labels):\n",
    "    label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    centroid_indices = np.array([label_to_index[label] if label in unique_labels else -1 for label in labels])\n",
    "    return centroid_indices\n",
    "\n",
    "def cluster_embeddings(embeddings, config):\n",
    "    distance_matrix = pairwise_distances(embeddings, metric='cosine')\n",
    "    Z = centroid_linkage_distance(distance_matrix)\n",
    "    labels = fcluster(Z, t=config['clustering']['threshold'], criterion='distance')\n",
    "    unique_labels, counts = np.unique(labels, return_counts=True)\n",
    "    valid_labels = unique_labels[counts >= config['clustering']['min_cluster_size']]\n",
    "    valid_indices = np.where(np.isin(labels, valid_labels))[0]\n",
    "    centroids, unique_labels = compute_centroids(embeddings, labels)\n",
    "    centroid_indices = map_labels_to_centroid_indices(labels, unique_labels)\n",
    "    valid_centroid_indices = np.array([index for index, centroid in enumerate(centroids) if index in np.unique(centroid_indices[valid_indices])])\n",
    "    return centroid_indices, centroids, valid_centroid_indices\n",
    "\n",
    "def find_largest_span(points):\n",
    "    largest_span_point = max(points, key=lambda p: p.payload['span'])\n",
    "    return largest_span_point\n",
    "\n",
    "def seconds_to_min_sec(seconds):\n",
    "    minutes = seconds // 60\n",
    "    seconds = seconds % 60\n",
    "    return f\"{int(minutes)}:{int(seconds):02d}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_redis_client(host: str, port: int, password=None):\n",
    "\n",
    "    # import redis.asyncio as aioredis\n",
    "    # redis_client = await aioredis.from_url(f\"redis://{host}:{port}/0\", decode_responses=True)\n",
    "    redis_client = Redis(host=host, port=port, password=password, decode_responses=True)\n",
    "    await redis_client.ping()\n",
    "\n",
    " \n",
    "\n",
    "    return redis_client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'clustering': {\n",
    "        'method': 'centroid',\n",
    "        'min_cluster_size': 3,\n",
    "        'threshold': 1.0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize Qdrant Client (Assuming already initialized as `client`)\n",
    "\n",
    "# Redis configuration\n",
    "redis = await get_redis_client('localhost', 63799,'')\n",
    "\n",
    "# Helper function to get Redis keys\n",
    "def get_redis_keys(meeting_id):\n",
    "    embeddings_key = f\"meeting:{meeting_id}:embeddings\"\n",
    "    centroids_key = f\"meeting:{meeting_id}:centroids\"\n",
    "    return embeddings_key, centroids_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def fetch_embeddings_and_centroids(redis, meeting_id):\n",
    "    embeddings_key, centroids_key = get_redis_keys(meeting_id)\n",
    "    embeddings = await redis.lrange(embeddings_key, 0, -1)\n",
    "    centroids = await redis.lrange(centroids_key, 0, -1)\n",
    "    return np.array([np.fromstring(e, sep=',') for e in embeddings]), np.array([np.fromstring(c, sep=',') for c in centroids])\n",
    "\n",
    "async def update_embeddings_and_centroids(redis, meeting_id, new_embeddings, new_centroids):\n",
    "    embeddings_key, centroids_key = get_redis_keys(meeting_id)\n",
    "    async with redis.pipeline(transaction=True) as pipe:\n",
    "        for emb in new_embeddings:\n",
    "            await pipe.rpush(embeddings_key, ','.join(map(str, emb)))\n",
    "        for cen in new_centroids:\n",
    "            await pipe.rpush(centroids_key, ','.join(map(str, cen)))\n",
    "        await pipe.execute()\n",
    "\n",
    "async def process_realtime_data(meeting_id, config):\n",
    "    diarizations = await speech_dal.get_all_diarizations(meeting_id)\n",
    "    embeddings_list = []\n",
    "    segments = []\n",
    "    \n",
    "    for diarization in diarizations:\n",
    "        segs = diarization['segments']\n",
    "        embs = diarization['embeddings']\n",
    "        embeddings_list.append(embs)\n",
    "        segments.append(pd.DataFrame(segs, columns=[\"start\", \"end\", \"speaker_id\"]))\n",
    "    \n",
    "    # Fetch current embeddings and centroids from Redis\n",
    "    current_embeddings, current_centroids = await fetch_embeddings_and_centroids(redis, meeting_id)\n",
    "\n",
    "    # Combine current and new embeddings\n",
    "    combined_embeddings = np.concatenate((current_embeddings, np.concatenate(embeddings_list, axis=0)), axis=0)\n",
    "\n",
    "    if combined_embeddings.shape[0] > 1:\n",
    "        clusters, centroids, valid_clusters = cluster_embeddings(combined_embeddings, config)\n",
    "        \n",
    "        # Update Redis with new centroids and embeddings\n",
    "        await update_embeddings_and_centroids(redis, meeting_id, combined_embeddings, centroids)\n",
    "        \n",
    "        segments_df = pd.concat(segments)\n",
    "        embs_mapping = segments_df[['speaker_id']].sort_values('speaker_id').drop_duplicates().set_index('speaker_id')\n",
    "        embs_mapping['emb_id'] = np.arange(len(embs_mapping))\n",
    "        embs_mapping['cluster'] = clusters\n",
    "        segments_df = segments_df.set_index('speaker_id').join(embs_mapping).set_index('emb_id').reset_index()\n",
    "        segments_df['span'] = segments_df['end'] - segments_df['start']\n",
    "        cluster_span = segments_df.groupby('cluster')[['span']].sum().to_dict()['span']\n",
    "        new_points = []\n",
    "        for cluster in cluster_span.keys():\n",
    "            point_id = None\n",
    "            centroid = centroids[cluster]\n",
    "            search_results = client.search(collection_name=\"speakers\", query_vector=list(centroid), limit=5, score_threshold=0.65)\n",
    "            \n",
    "            if len(search_results) == 0:\n",
    "                if cluster in valid_clusters:\n",
    "                    point_id = str(uuid4())\n",
    "                    new_points.append(PointStruct(id=point_id, vector=list(centroid), payload={\"span\": cluster_span[cluster]}))\n",
    "            else:\n",
    "                search_result = find_largest_span(search_results)\n",
    "                point_id = search_result.id \n",
    "                remote_span = search_result.payload['span']\n",
    "                span = cluster_span[cluster]\n",
    "                if span > remote_span:\n",
    "                    new_points.append(PointStruct(id=point_id, vector=list(centroid), payload={\"span\": span}))  \n",
    "            \n",
    "            segments_df.loc[segments_df['cluster'] == cluster, 'point_id'] = point_id\n",
    "        \n",
    "        if len(new_points) > 0:\n",
    "            client.upsert(collection_name=\"speakers\", wait=True, points=new_points)\n",
    "        \n",
    "        return segments_df\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'speech_dal' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m segments_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m process_realtime_data(meeting_id, config)\n",
      "Cell \u001b[0;32mIn[20], line 17\u001b[0m, in \u001b[0;36mprocess_realtime_data\u001b[0;34m(meeting_id, config)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_realtime_data\u001b[39m(meeting_id, config):\n\u001b[0;32m---> 17\u001b[0m     diarizations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mspeech_dal\u001b[49m\u001b[38;5;241m.\u001b[39mget_all_diarizations(meeting_id)\n\u001b[1;32m     18\u001b[0m     embeddings_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     19\u001b[0m     segments \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'speech_dal' is not defined"
     ]
    }
   ],
   "source": [
    "segments_df = await process_realtime_data(meeting_id, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      4\u001b[0m     meeting_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myour_meeting_id_here\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m     segments_df \u001b[38;5;241m=\u001b[39m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_realtime_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeeting_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m segments_df \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m         rename_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      9\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma1e24d98-918b-4633-8a4e-a4613949c0bc\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRyabenko\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1c6cac3c-0eba-4404-a6fc-ab1f54992292\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIkenna\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \n\u001b[1;32m     19\u001b[0m         }\n",
      "File \u001b[0;32m~/anaconda3/envs/pyannote/lib/python3.10/asyncio/runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "segments_df = asyncio.run(process_realtime_data(meeting_id, config))\n",
    "    \n",
    "if segments_df is not None:\n",
    "    rename_dict = {\n",
    "        'a1e24d98-918b-4633-8a4e-a4613949c0bc': 'Ryabenko',\n",
    "        '1c6cac3c-0eba-4404-a6fc-ab1f54992292': 'Ikenna',\n",
    "        'e92fd67c-d305-4bd4-a1d4-81dc86e2f063': 'Me',\n",
    "        'f39f85d8-2c76-4ee5-bb70-dc18f7c39af5': 'Me',\n",
    "        '7c0d4578-e6e2-4216-b3fa-bc9fa31162c9': 'Me',\n",
    "        '4384a580-e3d3-4b96-bd01-5412318bdbda': 'Andrew',\n",
    "        'd22658aa-ab7a-4aea-a322-ff86c2274b18': 'Olya',\n",
    "        'feed0fdf-8332-4c6c-842d-b8fcc1543508': 'Liza',\n",
    "        '77270ffb-54a5-4a23-8b10-d273347a7233': 'Egor',\n",
    "        \n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyannote",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
